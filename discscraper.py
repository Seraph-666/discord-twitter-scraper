import time
import os
import random
import re
import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.common.exceptions import StaleElementReferenceException
from selenium.webdriver.support import expected_conditions as EC
from colorama import Fore, Style, init

# === CONFIG ===
DISCORD_URL = "https://discord.com/channels/874638621368533012/916451487834599494"
OUTPUT_FILE = "twitter_links.txt"
CHROME_PROFILE_PATH = "C:/Users/admin1/AppData/Local/Google/Chrome/User Data"
PROFILE_NAME = "Default"

# === SETUP CHROME ===
init(autoreset=True)
options = uc.ChromeOptions()
options.add_argument(f"--user-data-dir={CHROME_PROFILE_PATH}")
options.add_argument(f"--profile-directory={PROFILE_NAME}")
driver = uc.Chrome(options=options)
driver.get(DISCORD_URL)

banner = r"""
‚ñì‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÑ  ‚ñà‚ñà‚ñì  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  ‚ñÑ‚ñà‚ñà‚ñà‚ñà‚ñÑ   ‚ñí‚ñà‚ñà‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñÄ‚ñà‚ñà‚ñà  ‚ñì‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÑ       
‚ñí‚ñà‚ñà‚ñÄ ‚ñà‚ñà‚ñå‚ñì‚ñà‚ñà‚ñí‚ñí‚ñà‚ñà    ‚ñí ‚ñí‚ñà‚ñà‚ñÄ ‚ñÄ‚ñà  ‚ñí‚ñà‚ñà‚ñí  ‚ñà‚ñà‚ñí‚ñì‚ñà‚ñà ‚ñí ‚ñà‚ñà‚ñí‚ñí‚ñà‚ñà‚ñÄ ‚ñà‚ñà‚ñå                    
‚ñë‚ñà‚ñà   ‚ñà‚ñå‚ñí‚ñà‚ñà‚ñí‚ñë ‚ñì‚ñà‚ñà‚ñÑ   ‚ñí‚ñì‚ñà    ‚ñÑ ‚ñí‚ñà‚ñà‚ñë  ‚ñà‚ñà‚ñí‚ñì‚ñà‚ñà ‚ñë‚ñÑ‚ñà ‚ñí‚ñë‚ñà‚ñà   ‚ñà‚ñå      
‚ñë‚ñì‚ñà‚ñÑ   ‚ñå‚ñë‚ñà‚ñà‚ñë  ‚ñí   ‚ñà‚ñà‚ñí‚ñí‚ñì‚ñì‚ñÑ ‚ñÑ‚ñà‚ñà‚ñí‚ñí‚ñà‚ñà   ‚ñà‚ñà‚ñë‚ñí‚ñà‚ñà‚ñÄ‚ñÄ‚ñà‚ñÑ  ‚ñë‚ñì‚ñà‚ñÑ   ‚ñå                
‚ñë‚ñí‚ñà‚ñà‚ñà‚ñà‚ñì ‚ñë‚ñà‚ñà‚ñë‚ñí‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñí‚ñí‚ñí ‚ñì‚ñà‚ñà‚ñà‚ñÄ ‚ñë‚ñë ‚ñà‚ñà‚ñà‚ñà‚ñì‚ñí‚ñë‚ñë‚ñà‚ñà‚ñì ‚ñí‚ñà‚ñà‚ñí‚ñë‚ñí‚ñà‚ñà‚ñà‚ñà‚ñì       
 ‚ñí‚ñí‚ñì  ‚ñí ‚ñë‚ñì  ‚ñí ‚ñí‚ñì‚ñí ‚ñí ‚ñë‚ñë ‚ñë‚ñí ‚ñí  ‚ñë‚ñë ‚ñí‚ñë‚ñí‚ñë‚ñí‚ñë ‚ñë ‚ñí‚ñì ‚ñë‚ñí‚ñì‚ñë ‚ñí‚ñí‚ñì  ‚ñí       
 ‚ñë ‚ñí  ‚ñí  ‚ñí ‚ñë‚ñë ‚ñë‚ñí  ‚ñë ‚ñë  ‚ñë  ‚ñí     ‚ñë ‚ñí ‚ñí‚ñë   ‚ñë‚ñí ‚ñë ‚ñí‚ñë ‚ñë ‚ñí  ‚ñí       
 ‚ñë ‚ñë  ‚ñë  ‚ñí ‚ñë‚ñë  ‚ñë  ‚ñë  ‚ñë        ‚ñë ‚ñë ‚ñë ‚ñí    ‚ñë‚ñë   ‚ñë  ‚ñë ‚ñë  ‚ñë       
   ‚ñë     ‚ñë        ‚ñë  ‚ñë ‚ñë          ‚ñë ‚ñë     ‚ñë        ‚ñë                    
 ‚ñë                   ‚ñë                           ‚ñë                 
"""
print(Fore.RED + banner + Style.RESET_ALL)
input("üü¢ Press ENTER when ready to begin scraping...")

twitter_links = set()

def normalize_link(link):
    link = link.replace("www.", "").replace("mobile.", "").rstrip("/")
    match = re.match(r"https?://(?:x|twitter)\.com/([^/]+)", link)
    if match:
        username = match.group(1)
        return f"https://x.com/{username}"
    return link

def try_open_full_bio():
    try:
        view_bio_button = WebDriverWait(driver, 1).until(
            EC.element_to_be_clickable((By.XPATH, '//button[contains(@class, "viewFullBio_f5f93a")]'))
        )
        driver.execute_script("arguments[0].click();", view_bio_button)
        time.sleep(random.uniform(0.2, 0.4))
        print("üìñ Clicked 'View Full Bio'.")
        return True
    except:
        return False

def extract_twitter():
    found_any = False

    try:
        # 1. Scrape connected accounts
        connections = driver.find_elements(By.XPATH, '//div[contains(@class, "connectedAccountNameText_e6abe8")]')
        for conn in connections:
            username = conn.get_attribute("aria-label")
            if username:
                link = normalize_link(f"https://x.com/{username}")
                if link not in twitter_links:
                    twitter_links.add(link)
                    print(f"‚úÖ Found (Connection): {link} | Total: {len(twitter_links)}")
                    found_any = True

        # 2. Scrape any visible Twitter bio links
        bio_links = driver.find_elements(By.XPATH, '//a[contains(@class, "anchor_edefb8") and (contains(@href, "twitter.com") or contains(@href, "x.com"))]')
        for link in bio_links:
            href = link.get_attribute("href")
            if href:
                clean_href = normalize_link(href)
                if clean_href not in twitter_links:
                    twitter_links.add(clean_href)
                    print(f"‚úÖ Found (Bio Link): {clean_href} | Total: {len(twitter_links)}")
                    found_any = True

        # 3. If nothing found, open Full Bio and try again
        if not found_any:
            if try_open_full_bio():
                time.sleep(0.5)
                connections = driver.find_elements(By.XPATH, '//div[contains(@class, "connectedAccountNameText_e6abe8")]')
                for conn in connections:
                    username = conn.get_attribute("aria-label")
                    if username:
                        link = normalize_link(f"https://x.com/{username}")
                        if link not in twitter_links:
                            twitter_links.add(link)
                            print(f"‚úÖ Found (Connection After Full Bio): {link} | Total: {len(twitter_links)}")

                bio_links = driver.find_elements(By.XPATH, '//a[contains(@class, "anchor_edefb8") and (contains(@href, "twitter.com") or contains(@href, "x.com"))]')
                for link in bio_links:
                    href = link.get_attribute("href")
                    if href:
                        clean_href = normalize_link(href)
                        if clean_href not in twitter_links:
                            twitter_links.add(clean_href)
                            print(f"‚úÖ Found (Bio Link After Full Bio): {clean_href} | Total: {len(twitter_links)}")

    except Exception as e:
        print(f"‚ùå Error scraping profile: {e}")

def close_modal():
    try:
        body = driver.find_element(By.TAG_NAME, 'body')
        body.send_keys(Keys.ESCAPE)
        time.sleep(random.uniform(0.2, 0.3))
    except:
        pass

def scrape_members_scroll_locked():
    print("üìú Starting scroll-locked scrape...")
    scroll_box = WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.XPATH, '//div[contains(@class, "members_c8ffbb")]'))
    )

    seen_ids = set()
    stuck_count = 0
    max_stuck_scrolls = 5

    while stuck_count < max_stuck_scrolls:
        members = driver.find_elements(By.XPATH, '//div[contains(@class, "member__5d473") and contains(@class, "clickable__91a9d")]')
        new_found = False

        for member in members:
            try:
                member_id = member.get_attribute("data-list-item-id")
                if member_id in seen_ids:
                    continue

                seen_ids.add(member_id)
                new_found = True

                driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", member)
                time.sleep(random.uniform(0.3, 0.5))
                member.click()
                time.sleep(random.uniform(0.3, 0.5))

                WebDriverWait(driver, 2).until(
                    EC.presence_of_element_located((By.XPATH, '//div[@role="dialog"]'))
                )
                extract_twitter()
                close_modal()

            except StaleElementReferenceException:
                pass
            except Exception as e:
                print(f"‚ùå Error on member: {e}")
                close_modal()

        if not new_found:
            stuck_count += 1
        else:
            stuck_count = 0

        driver.execute_script("arguments[0].scrollBy(0, 800);", scroll_box)
        time.sleep(random.uniform(0.3, 0.6))

    print(f"üèÅ Finished. Scraped {len(seen_ids)} unique members.")

# === MAIN EXECUTION ===
try:
    scrape_members_scroll_locked()
except KeyboardInterrupt:
    print("\nüõë CTRL+C detected. Saving progress...")

# === SAVE RESULTS CLEANLY ===
deduped_links = sorted(twitter_links)

with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
    for link in deduped_links:
        f.write(link + "\n")

print(f"‚úÖ Done. Saved {len(deduped_links)} links to {OUTPUT_FILE}")
driver.quit()
